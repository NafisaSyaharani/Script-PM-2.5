# Install library yang diperlukan
!pip install geopandas

# Import library
import geopandas as gpd
import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

# Load the dataset with all columns as strings, specifying the encoding
df = pd.read_csv('/content/drive/MyDrive/TA_PM2.5/Tabel Uji Normalitas Ver8.csv', delimiter=';', dtype=str)

# Strip whitespace from column headers
df.columns = df.columns.str.strip()

# Display the data
print(df.head())
# Clean and convert relevant columns to numeric types
df['PM2.5'] = pd.to_numeric(df['PM2.5'].replace('\t', '', regex=True), errors='coerce') # Use .replace instead of .str.replace for non-string columns
df['Traffic'] = pd.to_numeric(df['Traffic'].replace('\t', '', regex=True), errors='coerce')
df['Wind Speed'] = pd.to_numeric(df['Wind Speed'].replace('\t', '', regex=True), errors='coerce')
df['Humidity'] = pd.to_numeric(df['Humidity'].replace('\t', '', regex=True), errors='coerce')
df['Temperature'] = pd.to_numeric(df['Temperature'].replace('\t', '', regex=True), errors='coerce')
df['Precipitation'] = pd.to_numeric(df['Precipitation'].replace('\t', '', regex=True), errors='coerce')
df['NDVI'] = pd.to_numeric(df['NDVI'].replace('\t', '', regex=True), errors='coerce')

# Display the cleaned DataFrame
print(df.head())
print(len(df))

# Normalisasi Data
df['PM2.5_rescale'] = (df['PM2.5'].astype(float) - df['PM2.5'].astype(float).min()) / (df['PM2.5'].astype(float).max() - df['PM2.5'].astype(float).min())
df['Traffic_rescale'] = (df['Traffic'].astype(float) - df['Traffic'].astype(float).min()) / (df['Traffic'].astype(float).max() - df['Traffic'].astype(float).min())
df['Wind Speed_rescale'] = (df['Wind Speed'].astype(float) - df['Wind Speed'].astype(float).min()) / (df['Wind Speed'].astype(float).max() - df['Wind Speed'].astype(float).min())
df['Humidity_rescale'] = (df['Humidity'].astype(float) - df['Humidity'].astype(float).min()) / (df['Humidity'].astype(float).max() - df['Humidity'].astype(float).min())
df['Temperature_rescale'] = (df['Temperature'].astype(float) - df['Temperature'].astype(float).min()) / (df['Temperature'].astype(float).max() - df['Temperature'].astype(float).min())
df['Precipitation_rescale'] = (df['Precipitation'].astype(float) - df['Precipitation'].astype(float).min()) / (df['Precipitation'].astype(float).max() - df['Precipitation'].astype(float).min())
df['NDVI_rescale'] = (df['NDVI'].astype(float) - df['NDVI'].astype(float).min()) / (df['NDVI'].astype(float).max() - df['NDVI'].astype(float).min())

# Function to remove outliers using IQR
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Remove outliers from 'data1'
df_clean1 = remove_outliers(df, 'PM2.5_rescale')
df_clean1 = remove_outliers(df_clean1, 'Traffic_rescale')
df_clean1 = remove_outliers(df_clean1, 'Precipitation_rescale')


# Summary of data cleaning
original_size1 = df.shape[0]
cleaned_size1 = df_clean1.shape[0]
removed_df1 = original_size1 - cleaned_size1

original_size1 = f"The total record before was {original_size1:}"
cleaned_size1 = f"The cleaned record is {cleaned_size1:}"
removed_df1 = f"The number of deleted record is {removed_df1:}"


original_size1, cleaned_size1, removed_df1

Subset Code By Area
# Group by the 'Code' column
grouped_df = df.groupby('Kode').mean()

import matplotlib.pyplot as plt
grouped_df[['PM2.5_rescale']].plot(kind='bar')
plt.xlabel('Kode')
plt.xticks(rotation=0)
plt.xticks(rotation=20, ha='right')
plt.ylabel('PM2.5_rescale')
plt.show()

import matplotlib.pyplot as plt
# Boxplot for the 'PM2.5'
plt.figure(figsize=(8, 6))
plt.boxplot(df['PM2.5_rescale'])
plt.title("Boxplot of PM2.5")
plt.xlabel("PM2.5_rescale")
plt.ylabel("Values")
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# Reading the CSV file
dataset = df
dataset.head()
# Extracting feature and target values
# Assuming that 'PM2.5' is the target column and dropping 'Kode'
X = df_clean1.drop(columns=['PM2.5_rescale','PM2.5','Traffic', 'Wind Speed','Humidity','Temperature','Precipitation','NDVI','Kode'])
y = df_clean1['PM2.5_rescale']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=41)

# Initialize models
linear_reg = LinearRegression()
random_forest = RandomForestRegressor(random_state=40)
xgboost_model = xgb.XGBRegressor(random_state=40)

# Function to train and evaluate a model
def train_and_evaluate(model, model_name, X_train, y_train, X_test, y_test):
    # Train the model
    model.fit(X_train, y_train)

    # Predict on test set
    y_pred = model.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    return {
        'model': model_name,
        'MSE': mse,
        'R2': r2
    }

# Train and evaluate Linear Regression
results = []
results.append(train_and_evaluate(linear_reg, 'Linear Regression', X_train, y_train, X_test, y_test))

# Train and evaluate Random Forest
results.append(train_and_evaluate(random_forest, 'Random Forest', X_train, y_train, X_test, y_test))

# Train and evaluate XGBoost
results.append(train_and_evaluate(xgboost_model, 'XGBoost', X_train, y_train, X_test, y_test))

# Display the results
print(pd.DataFrame(results))

K-FOLD CROSS VALIDATION
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score

# Number of folds
n_splits = 5

# Initialize KFold cross-validator
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Function to train and evaluate a model with K-Fold cross-validation
def train_and_evaluate_kfold(model, model_name, X, y, kf):
    results_per_fold = []
    for fold, (train_index, test_index) in enumerate(kf.split(X)):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        results_per_fold.append({
            'model': model_name,
            'Fold': fold + 1,
            'MSE': mse,
            'R2': r2
        })

    return results_per_fold


# Perform K-Fold cross-validation for each model
kfold_results = []
kfold_results.extend(train_and_evaluate_kfold(linear_reg, 'Linear Regression', X, y, kf))
kfold_results.extend(train_and_evaluate_kfold(random_forest, 'Random Forest', X, y, kf))
kfold_results.extend(train_and_evaluate_kfold(xgboost_model, 'XGBoost', X, y, kf))

# Display the results for each fold
kfold_df = pd.DataFrame(kfold_results)
print(kfold_df)


Variable of Importance for Each Area
# Group data by 'Kode'
grouped_data = df_clean1.groupby('Kode')

feature_importance_by_kode = {}

for kode, group in grouped_data:
  # Extract features and target for the current 'Kode'
  X_kode = group.drop(columns=['PM2.5_rescale','PM2.5','Traffic', 'Wind Speed','Humidity','Temperature','Precipitation','NDVI','Kode'])
  y_kode = group['PM2.5_rescale']

  # If there's not enough data for the current 'Kode', skip it
  if len(X_kode) < 2:
    continue

  # Train a Random Forest model for the current 'Kode'
  rf_model = RandomForestRegressor(random_state=40)
  rf_model.fit(X_kode, y_kode)

  # Get feature importances
  feature_importances = rf_model.feature_importances_

  # Store feature importances for the current 'Kode'
  feature_importance_by_kode[kode] = dict(zip(X_kode.columns, feature_importances))

# Print or analyze the feature importance for each 'Kode'
for kode, feature_importance in feature_importance_by_kode.items():
  print(f"Feature Importance for Kode {kode}:")
  for feature, importance in feature_importance.items():
    print(f"  {feature}: {importance}")
  print("-" * 20)


